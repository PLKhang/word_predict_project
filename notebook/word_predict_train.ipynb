{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8027f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file văn bản từ data/train_data.txt\n",
    "with open(\"../data/test.vi\", encoding=\"utf-8\") as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "# Làm sạch và xử lý\n",
    "def clean(text): return text.strip().lower()\n",
    "texts = [clean(line) for line in texts if len(line.strip()) > 0]\n",
    "\n",
    "# Lưu lại file đã xử lý\n",
    "with open(\"cleaned_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in texts:\n",
    "        f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92845cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # thêm pad_token nếu thiếu\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"cleaned_data.txt\",\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/gpt2-word-predict\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e18bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/gpt2-word-predict\")\n",
    "tokenizer.save_pretrained(\"../models/gpt2-word-predict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2766bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictor import WordPredictor\n",
    "\n",
    "predictor = WordPredictor(\"../models/gpt2-word-predict\")\n",
    "predictor.suggest_next(\"next m\", top_k=5, prefix_filter=\"m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10386f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "class WordPredictor:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def suggest_next(self, prompt, top_k=5, prefix_filter=None):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            top_k_ids = logits.topk(top_k * 2).indices\n",
    "            candidates = [self.tokenizer.decode([i]).strip() for i in top_k_ids]\n",
    "            if prefix_filter:\n",
    "                candidates = [c for c in candidates if c.startswith(prefix_filter)]\n",
    "            return candidates[:top_k]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
